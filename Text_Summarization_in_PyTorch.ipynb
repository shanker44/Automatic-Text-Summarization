{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization in PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0POEYiSL4BDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ef908e5a-6852-48d4-bcfb-880949cbfb3a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import spacy\n",
        "import math\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuqAH5BV3lwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "0fa0d101-66a1-4d2c-efdd-c38541d2df6b"
      },
      "source": [
        "while 1:\n",
        "    x = np.array([1,2,3])\n",
        "    y = np.array([1,2,3])\n",
        "    z = x * y.T"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-75f507c67e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I71U082oG69x",
        "colab_type": "text"
      },
      "source": [
        "# **Loading and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcJ7i-6J37cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "036f4e84-3f11-4ccc-e327-a4fbd21f8e74"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/news_summary_more.csv\")\n",
        "data.head(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rahat Fateh Ali Khan denies getting notice for...</td>\n",
              "      <td>Pakistani singer Rahat Fateh Ali Khan has deni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>India get all out for 92, their lowest ODI tot...</td>\n",
              "      <td>India recorded their lowest ODI total in New Z...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Govt directs Alok Verma to join work 1 day bef...</td>\n",
              "      <td>Weeks after ex-CBI Director Alok Verma told th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Called PM Modi 'sir' 10 times to satisfy his e...</td>\n",
              "      <td>Andhra Pradesh CM N Chandrababu Naidu has said...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Cong wins Ramgarh bypoll in Rajasthan, takes t...</td>\n",
              "      <td>Congress candidate Shafia Zubair won the Ramga...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio...\n",
              "5  Rahat Fateh Ali Khan denies getting notice for...  Pakistani singer Rahat Fateh Ali Khan has deni...\n",
              "6  India get all out for 92, their lowest ODI tot...  India recorded their lowest ODI total in New Z...\n",
              "7  Govt directs Alok Verma to join work 1 day bef...  Weeks after ex-CBI Director Alok Verma told th...\n",
              "8  Called PM Modi 'sir' 10 times to satisfy his e...  Andhra Pradesh CM N Chandrababu Naidu has said...\n",
              "9  Cong wins Ramgarh bypoll in Rajasthan, takes t...  Congress candidate Shafia Zubair won the Ramga..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dt38Lge2BII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, valid = train_test_split(data, test_size = 0.1, shuffle = True)\n",
        "valid, test = train_test_split(valid, test_size = 0.1, shuffle = True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5O9sHyy2f-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(\"train.csv\", index = False)\n",
        "valid.to_csv(\"valid.csv\", index = False)\n",
        "test.to_csv(\"test.csv\", index = False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Zp4Lwe5DWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines = data['headlines'].tolist()\n",
        "text = data['text'].tolist()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SriTuxcJ4Ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline_length = [len(x.split()) for x in headlines]\n",
        "text_length = [len(x.split()) for x in text]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrHItUtLKyRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "f543da33-c7ba-463d-9403-18b92fd5381c"
      },
      "source": [
        "plt.hist(headline_length, bins = 10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.0000e+00, 1.4000e+01, 1.4840e+03, 5.4910e+03, 4.1552e+04,\n",
              "        4.1212e+04, 6.3630e+03, 2.2060e+03, 7.7000e+01, 1.0000e+00]),\n",
              " array([ 1. ,  2.7,  4.4,  6.1,  7.8,  9.5, 11.2, 12.9, 14.6, 16.3, 18. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5vuyNEOE19I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "3fa262f9-c0cf-49d1-fae3-302297d9925a"
      },
      "source": [
        "plt.hist(text_length, bins = 15)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 2.2000e+01,\n",
              "        2.4400e+02, 3.3620e+03, 2.2697e+04, 7.2036e+04, 3.8000e+01]),\n",
              " array([ 1.        ,  5.33333333,  9.66666667, 14.        , 18.33333333,\n",
              "        22.66666667, 27.        , 31.33333333, 35.66666667, 40.        ,\n",
              "        44.33333333, 48.66666667, 53.        , 57.33333333, 61.66666667,\n",
              "        66.        ]),\n",
              " <a list of 15 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl-mGEXgLMi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9R5AsUL3cO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source = Field(sequential = True, use_vocab = True, init_token = '<sos>', eos_token = '<eos>', lower = True, tokenize = 'spacy')\n",
        "target = Field(sequential = True, use_vocab = True, init_token = '<sos>', eos_token = '<eos>', lower = True, tokenize = 'spacy')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOIt6-Q4_bsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, valid, test = TabularDataset.splits(path = '/content', train = 'train.csv', validation = 'valid.csv', test = 'test.csv',\n",
        "                                           format = 'csv', fields = [('headlines', target), ('text', source)])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViLFHugVouwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source.build_vocab(train, max_size = 10000, min_freq = 2)\n",
        "target.build_vocab(train, max_size = 10000, min_freq = 2)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLtm6hN-75_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgqOBjLaG069",
        "colab_type": "text"
      },
      "source": [
        "# **Building the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO55EK3aGWv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We are creating a model where the encoder hidden states and the decoder hidden states have the same dimension\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedd_dim, hidden_dim, n_layers, drop_prob):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedd_dim = embedd_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embed = nn.Embedding(input_dim, embedd_dim)\n",
        "\n",
        "        self.gru = nn.GRU(embedd_dim, hidden_dim, num_layers = n_layers, dropout = drop_prob, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = drop_prob)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #shape of x = [sentence_length, batch_size]\n",
        "        \n",
        "        embedding = self.dropout(self.embed(x))\n",
        "        #shape of embedding = [sentence_len, batch_size, embedd_dim]\n",
        "\n",
        "        encoder_state, hidden = self.gru(embedding)\n",
        "        #shape of encoder_state = [sentence_len, batch_size, hidden_dim]\n",
        "        #shape of hidden = [num_directions * num_layers, batch_size, hidden_dim] ->is of the last time step\n",
        "\n",
        "        hidden = self.fc(torch.cat((hidden[0:1], hidden[1:2]), dim = 2))\n",
        "        #since hidden shape will be [2, batch_size, hidden_dim] and we want it to be [batch_size, hidden_dim*2] so that it can be passed to the fully connected layer\n",
        "        #nn.GRU()\n",
        "        return encoder_state, hidden"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k47fIkXeOSiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedd_dim, hidden_dim, output_dim, n_layers, drop_prob):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.embedd_dim = embedd_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "        self.embed = nn.Embedding(input_dim, embedd_dim)\n",
        "        \n",
        "        self.gru = nn.GRU(embedd_dim + hidden_dim*2, hidden_dim, num_layers = n_layers)\n",
        "        #embedd_dim for the input word and hidden_dim from the context produced by the encoder\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(p = drop_prob)\n",
        "        self.energy = nn.Linear(hidden_dim*3, 1) #2 from the encoder state and 1 from previous decoder\n",
        "        self.softmax = nn.Softmax(dim = 0)\n",
        "        self.relu = nn.ReLU()      \n",
        "\n",
        "    def forward(self, x, encoder_state, hidden):\n",
        "        #x will be a word per batch, i.e. shape of x = [batch_size], but we want it to be [1, batch_size] so as to explicitly state 1 word per batch\n",
        "        x = x.unsqueeze(dim = 0)\n",
        "\n",
        "        embedd = self.dropout(self.embed(x))\n",
        "        #embedd shape = [1, batch_size, embedd_dim]\n",
        "        #hidden shape = [batch_size, hidden_dim]\n",
        "        #encoder_state shape = [sentence_len, batch_size, hidden_dim*2]\n",
        "\n",
        "        sentence_length = encoder_state.shape[0]\n",
        "        hidden_reshaped = hidden.repeat(sentence_length, 1, 1)\n",
        "        #hidden_reshaped shape = [sentence_length, batch_size, hidden_dim]\n",
        "\n",
        "        energy = self.relu(self.energy(torch.cat((hidden_reshaped, encoder_state), dim = 2)))\n",
        "        #energy shape = [sentence_len, batch_size, 1]\n",
        "\n",
        "        attention = self.softmax(energy)\n",
        "        #attention shape = [sentence_len, batch_size, 1]\n",
        "\n",
        "        attention = attention.permute(1, 2, 0)\n",
        "        #attention = [batch_size, 1, sentence_len]\n",
        "        encoder_state = encoder_state.permute(1, 0, 2)\n",
        "        #encoder_state = [batch_size, sentence_len, hidden_dim*2]\n",
        "\n",
        "        context_vector = torch.bmm(attention, encoder_state).permute(1, 0, 2)\n",
        "        #context_vector = [1, batch_size, hidden_dim*2]\n",
        "\n",
        "        rnn_input = torch.cat((context_vector, embedd), dim = 2)\n",
        "        #rnn_input = [1, batch_size, embedd_dim + hidden_dim*2]\n",
        "\n",
        "        outputs, hidden = self.gru(rnn_input, hidden) \n",
        "        #outputs = [1, batch_size, hidden_dim]\n",
        "\n",
        "        predictions = self.fc(outputs).squeeze(dim = 0)\n",
        "        #predictions = [batch_size, output_dim]\n",
        "\n",
        "        return predictions, hidden"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM6OnPFPosab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source_sentence, target_sentence, teacher_forcing_ratio = 0.5):\n",
        "        #source_sentence = [sentence_len, batch_size]\n",
        "        #target_sentence = [target_len, batch_size]\n",
        "\n",
        "        batch_size = source_sentence.shape[1]\n",
        "        target_len = target_sentence.shape[0]\n",
        "        target_vocab_size = len(target.vocab)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        #for each batch, we want a target sentence of length target len, and each of those positions will have some probability for each word\n",
        "\n",
        "        encoder_state, hidden = self.encoder(source_sentence)\n",
        "\n",
        "        first_word = target_sentence[0]\n",
        "        #first word is <SOS> token\n",
        "\n",
        "        for word in range(1, target_len):\n",
        "            output, hidden = self.decoder(first_word, encoder_state, hidden)\n",
        "\n",
        "            outputs[word] = output\n",
        "\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            x = target_sentence[word] if random.random() < teacher_forcing_ratio else best_guess\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LClxuIhtuUqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 30\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "\n",
        "input_size_encoder = len(source.vocab)\n",
        "input_size_decoder = len(target.vocab)\n",
        "output_size = len(target.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "enc_dropout = 0\n",
        "dec_dropout = 0"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m53UVfA7uv8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "train_iter, valid_iter, test_iter = BucketIterator.splits((train, valid, test), batch_sizes = (batch_size, batch_size, batch_size), sort_within_batch = True, sort_key = lambda x : len(x.headlines), device = device)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHP1wwFFvJ_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout)\n",
        "decoder = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFgrmE00zEkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfGIN6HUzVin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = target.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c1-gHHqzfnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b35c25ec-9455-4012-9d63-a35fb43d2376"
      },
      "source": [
        "step = 0\n",
        "print_every = 100\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        input_data = batch.text.to(device)\n",
        "        targ = batch.headlines.to(device)\n",
        "\n",
        "        output = model(input_data, targ)\n",
        "\n",
        "        #output is of shape [target_len, batch_size, output_dim] but cross entropy doesn't accept in that form, so we reshape it as follows\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        targ = targ[1:].reshape(-1)\n",
        "        #we're doing [1:] so as to not consider the start token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, targ)\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 2)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "        if step % print_every == 0:\n",
        "            print(f\"[Step : {step}, '\\t', Loss : {loss.item()/batch_size}]\")    "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1 / 30]\n",
            "[Step : 100, '\t', Loss : 0.21468855440616608]\n",
            "[Step : 200, '\t', Loss : 0.22141076624393463]\n",
            "[Step : 300, '\t', Loss : 0.20844396948814392]\n",
            "[Step : 400, '\t', Loss : 0.19881239533424377]\n",
            "[Step : 500, '\t', Loss : 0.19567663967609406]\n",
            "[Step : 600, '\t', Loss : 0.18355108797550201]\n",
            "[Step : 700, '\t', Loss : 0.19012056291103363]\n",
            "[Step : 800, '\t', Loss : 0.185635045170784]\n",
            "[Step : 900, '\t', Loss : 0.17785464227199554]\n",
            "[Step : 1000, '\t', Loss : 0.1810442954301834]\n",
            "[Step : 1100, '\t', Loss : 0.17285865545272827]\n",
            "[Step : 1200, '\t', Loss : 0.18881745636463165]\n",
            "[Step : 1300, '\t', Loss : 0.17438724637031555]\n",
            "[Step : 1400, '\t', Loss : 0.18037399649620056]\n",
            "[Step : 1500, '\t', Loss : 0.17238134145736694]\n",
            "[Step : 1600, '\t', Loss : 0.17071887850761414]\n",
            "[Step : 1700, '\t', Loss : 0.16919410228729248]\n",
            "[Step : 1800, '\t', Loss : 0.16184963285923004]\n",
            "[Step : 1900, '\t', Loss : 0.17529553174972534]\n",
            "[Step : 2000, '\t', Loss : 0.1719479113817215]\n",
            "[Step : 2100, '\t', Loss : 0.16446921229362488]\n",
            "[Step : 2200, '\t', Loss : 0.16205789148807526]\n",
            "[Step : 2300, '\t', Loss : 0.15681160986423492]\n",
            "[Step : 2400, '\t', Loss : 0.15577585995197296]\n",
            "[Step : 2500, '\t', Loss : 0.15968936681747437]\n",
            "[Step : 2600, '\t', Loss : 0.16047485172748566]\n",
            "[Step : 2700, '\t', Loss : 0.1520654559135437]\n",
            "[Epoch 2 / 30]\n",
            "[Step : 2800, '\t', Loss : 0.14812754094600677]\n",
            "[Step : 2900, '\t', Loss : 0.1454365998506546]\n",
            "[Step : 3000, '\t', Loss : 0.14898009598255157]\n",
            "[Step : 3100, '\t', Loss : 0.14474022388458252]\n",
            "[Step : 3200, '\t', Loss : 0.1388782560825348]\n",
            "[Step : 3300, '\t', Loss : 0.14136196672916412]\n",
            "[Step : 3400, '\t', Loss : 0.15217554569244385]\n",
            "[Step : 3500, '\t', Loss : 0.14616833627223969]\n",
            "[Step : 3600, '\t', Loss : 0.14690613746643066]\n",
            "[Step : 3700, '\t', Loss : 0.14144498109817505]\n",
            "[Step : 3800, '\t', Loss : 0.13654834032058716]\n",
            "[Step : 3900, '\t', Loss : 0.1378547102212906]\n",
            "[Step : 4000, '\t', Loss : 0.12359178066253662]\n",
            "[Step : 4100, '\t', Loss : 0.15079636871814728]\n",
            "[Step : 4200, '\t', Loss : 0.13399843871593475]\n",
            "[Step : 4300, '\t', Loss : 0.13798977434635162]\n",
            "[Step : 4400, '\t', Loss : 0.1420574188232422]\n",
            "[Step : 4500, '\t', Loss : 0.13506698608398438]\n",
            "[Step : 4600, '\t', Loss : 0.14001226425170898]\n",
            "[Step : 4700, '\t', Loss : 0.14743192493915558]\n",
            "[Step : 4800, '\t', Loss : 0.13445177674293518]\n",
            "[Step : 4900, '\t', Loss : 0.15669676661491394]\n",
            "[Step : 5000, '\t', Loss : 0.1381484866142273]\n",
            "[Step : 5100, '\t', Loss : 0.15265445411205292]\n",
            "[Step : 5200, '\t', Loss : 0.1397099792957306]\n",
            "[Step : 5300, '\t', Loss : 0.13321323692798615]\n",
            "[Step : 5400, '\t', Loss : 0.1468399167060852]\n",
            "[Step : 5500, '\t', Loss : 0.1372554451227188]\n",
            "[Epoch 3 / 30]\n",
            "[Step : 5600, '\t', Loss : 0.1092301607131958]\n",
            "[Step : 5700, '\t', Loss : 0.11443882435560226]\n",
            "[Step : 5800, '\t', Loss : 0.11860428005456924]\n",
            "[Step : 5900, '\t', Loss : 0.11429724097251892]\n",
            "[Step : 6000, '\t', Loss : 0.1207638680934906]\n",
            "[Step : 6100, '\t', Loss : 0.12756071984767914]\n",
            "[Step : 6200, '\t', Loss : 0.11661850661039352]\n",
            "[Step : 6300, '\t', Loss : 0.11293347924947739]\n",
            "[Step : 6400, '\t', Loss : 0.12460273504257202]\n",
            "[Step : 6500, '\t', Loss : 0.11822842806577682]\n",
            "[Step : 6600, '\t', Loss : 0.1131954938173294]\n",
            "[Step : 6700, '\t', Loss : 0.1256624311208725]\n",
            "[Step : 6800, '\t', Loss : 0.11133445054292679]\n",
            "[Step : 6900, '\t', Loss : 0.12739571928977966]\n",
            "[Step : 7000, '\t', Loss : 0.11628684401512146]\n",
            "[Step : 7100, '\t', Loss : 0.13447882235050201]\n",
            "[Step : 7200, '\t', Loss : 0.13435529172420502]\n",
            "[Step : 7300, '\t', Loss : 0.1061902642250061]\n",
            "[Step : 7400, '\t', Loss : 0.12081574648618698]\n",
            "[Step : 7500, '\t', Loss : 0.1326771378517151]\n",
            "[Step : 7600, '\t', Loss : 0.12169179320335388]\n",
            "[Step : 7700, '\t', Loss : 0.11764828860759735]\n",
            "[Step : 7800, '\t', Loss : 0.12734492123126984]\n",
            "[Step : 7900, '\t', Loss : 0.12809588015079498]\n",
            "[Step : 8000, '\t', Loss : 0.11475416272878647]\n",
            "[Step : 8100, '\t', Loss : 0.12359940260648727]\n",
            "[Step : 8200, '\t', Loss : 0.12350861728191376]\n",
            "[Step : 8300, '\t', Loss : 0.11975062638521194]\n",
            "[Epoch 4 / 30]\n",
            "[Step : 8400, '\t', Loss : 0.08640819042921066]\n",
            "[Step : 8500, '\t', Loss : 0.10215838998556137]\n",
            "[Step : 8600, '\t', Loss : 0.09988377243280411]\n",
            "[Step : 8700, '\t', Loss : 0.08291547745466232]\n",
            "[Step : 8800, '\t', Loss : 0.08809733390808105]\n",
            "[Step : 8900, '\t', Loss : 0.09716014564037323]\n",
            "[Step : 9000, '\t', Loss : 0.09392677992582321]\n",
            "[Step : 9100, '\t', Loss : 0.08803106099367142]\n",
            "[Step : 9200, '\t', Loss : 0.11200492829084396]\n",
            "[Step : 9300, '\t', Loss : 0.09922048449516296]\n",
            "[Step : 9400, '\t', Loss : 0.09021517634391785]\n",
            "[Step : 9500, '\t', Loss : 0.10641128569841385]\n",
            "[Step : 9600, '\t', Loss : 0.12097891420125961]\n",
            "[Step : 9700, '\t', Loss : 0.10568119585514069]\n",
            "[Step : 9800, '\t', Loss : 0.1041010171175003]\n",
            "[Step : 9900, '\t', Loss : 0.1300891935825348]\n",
            "[Step : 10000, '\t', Loss : 0.09678185731172562]\n",
            "[Step : 10100, '\t', Loss : 0.09879544377326965]\n",
            "[Step : 10200, '\t', Loss : 0.09265906363725662]\n",
            "[Step : 10300, '\t', Loss : 0.12171709537506104]\n",
            "[Step : 10400, '\t', Loss : 0.10640338063240051]\n",
            "[Step : 10500, '\t', Loss : 0.11202598363161087]\n",
            "[Step : 10600, '\t', Loss : 0.10688698291778564]\n",
            "[Step : 10700, '\t', Loss : 0.10209184139966965]\n",
            "[Step : 10800, '\t', Loss : 0.10610681772232056]\n",
            "[Step : 10900, '\t', Loss : 0.12744034826755524]\n",
            "[Step : 11000, '\t', Loss : 0.09823098033666611]\n",
            "[Epoch 5 / 30]\n",
            "[Step : 11100, '\t', Loss : 0.10674574971199036]\n",
            "[Step : 11200, '\t', Loss : 0.0702473372220993]\n",
            "[Step : 11300, '\t', Loss : 0.08634313195943832]\n",
            "[Step : 11400, '\t', Loss : 0.08352205902338028]\n",
            "[Step : 11500, '\t', Loss : 0.07521993666887283]\n",
            "[Step : 11600, '\t', Loss : 0.08728650957345963]\n",
            "[Step : 11700, '\t', Loss : 0.10168318450450897]\n",
            "[Step : 11800, '\t', Loss : 0.0836416482925415]\n",
            "[Step : 11900, '\t', Loss : 0.08024073392152786]\n",
            "[Step : 12000, '\t', Loss : 0.06551521271467209]\n",
            "[Step : 12100, '\t', Loss : 0.07972961664199829]\n",
            "[Step : 12200, '\t', Loss : 0.08095183968544006]\n",
            "[Step : 12300, '\t', Loss : 0.09977186471223831]\n",
            "[Step : 12400, '\t', Loss : 0.06942018866539001]\n",
            "[Step : 12500, '\t', Loss : 0.09890515357255936]\n",
            "[Step : 12600, '\t', Loss : 0.07825420051813126]\n",
            "[Step : 12700, '\t', Loss : 0.07386527210474014]\n",
            "[Step : 12800, '\t', Loss : 0.11410925537347794]\n",
            "[Step : 12900, '\t', Loss : 0.07951456308364868]\n",
            "[Step : 13000, '\t', Loss : 0.08047724515199661]\n",
            "[Step : 13100, '\t', Loss : 0.07993301749229431]\n",
            "[Step : 13200, '\t', Loss : 0.09937427937984467]\n",
            "[Step : 13300, '\t', Loss : 0.1006917804479599]\n",
            "[Step : 13400, '\t', Loss : 0.09231682866811752]\n",
            "[Step : 13500, '\t', Loss : 0.07734370976686478]\n",
            "[Step : 13600, '\t', Loss : 0.11261282861232758]\n",
            "[Step : 13700, '\t', Loss : 0.07952315360307693]\n",
            "[Step : 13800, '\t', Loss : 0.07353617250919342]\n",
            "[Epoch 6 / 30]\n",
            "[Step : 13900, '\t', Loss : 0.06885366886854172]\n",
            "[Step : 14000, '\t', Loss : 0.09844603389501572]\n",
            "[Step : 14100, '\t', Loss : 0.08076906204223633]\n",
            "[Step : 14200, '\t', Loss : 0.07088740915060043]\n",
            "[Step : 14300, '\t', Loss : 0.07350301742553711]\n",
            "[Step : 14400, '\t', Loss : 0.07416930794715881]\n",
            "[Step : 14500, '\t', Loss : 0.09252481907606125]\n",
            "[Step : 14600, '\t', Loss : 0.0578296072781086]\n",
            "[Step : 14700, '\t', Loss : 0.06400222331285477]\n",
            "[Step : 14800, '\t', Loss : 0.10903490334749222]\n",
            "[Step : 14900, '\t', Loss : 0.06246902421116829]\n",
            "[Step : 15000, '\t', Loss : 0.05789930745959282]\n",
            "[Step : 15100, '\t', Loss : 0.07432626932859421]\n",
            "[Step : 15200, '\t', Loss : 0.09001917392015457]\n",
            "[Step : 15300, '\t', Loss : 0.08684170991182327]\n",
            "[Step : 15400, '\t', Loss : 0.07003141194581985]\n",
            "[Step : 15500, '\t', Loss : 0.08711527287960052]\n",
            "[Step : 15600, '\t', Loss : 0.06560486555099487]\n",
            "[Step : 15700, '\t', Loss : 0.06620324403047562]\n",
            "[Step : 15800, '\t', Loss : 0.07873786240816116]\n",
            "[Step : 15900, '\t', Loss : 0.10100607573986053]\n",
            "[Step : 16000, '\t', Loss : 0.06675297021865845]\n",
            "[Step : 16100, '\t', Loss : 0.08485863357782364]\n",
            "[Step : 16200, '\t', Loss : 0.06086389347910881]\n",
            "[Step : 16300, '\t', Loss : 0.07497192174196243]\n",
            "[Step : 16400, '\t', Loss : 0.0768098458647728]\n",
            "[Step : 16500, '\t', Loss : 0.08711390197277069]\n",
            "[Step : 16600, '\t', Loss : 0.0776422992348671]\n",
            "[Epoch 7 / 30]\n",
            "[Step : 16700, '\t', Loss : 0.06969603896141052]\n",
            "[Step : 16800, '\t', Loss : 0.06697966903448105]\n",
            "[Step : 16900, '\t', Loss : 0.06493950635194778]\n",
            "[Step : 17000, '\t', Loss : 0.08211340010166168]\n",
            "[Step : 17100, '\t', Loss : 0.059999559074640274]\n",
            "[Step : 17200, '\t', Loss : 0.07623892277479172]\n",
            "[Step : 17300, '\t', Loss : 0.05499574542045593]\n",
            "[Step : 17400, '\t', Loss : 0.06923200935125351]\n",
            "[Step : 17500, '\t', Loss : 0.0709037259221077]\n",
            "[Step : 17600, '\t', Loss : 0.09862034767866135]\n",
            "[Step : 17700, '\t', Loss : 0.08265241235494614]\n",
            "[Step : 17800, '\t', Loss : 0.055783215910196304]\n",
            "[Step : 17900, '\t', Loss : 0.06684625148773193]\n",
            "[Step : 18000, '\t', Loss : 0.05129988491535187]\n",
            "[Step : 18100, '\t', Loss : 0.07483664900064468]\n",
            "[Step : 18200, '\t', Loss : 0.06236230209469795]\n",
            "[Step : 18300, '\t', Loss : 0.06540894508361816]\n",
            "[Step : 18400, '\t', Loss : 0.06335172057151794]\n",
            "[Step : 18500, '\t', Loss : 0.07064474374055862]\n",
            "[Step : 18600, '\t', Loss : 0.08127031475305557]\n",
            "[Step : 18700, '\t', Loss : 0.07391904294490814]\n",
            "[Step : 18800, '\t', Loss : 0.06525799632072449]\n",
            "[Step : 18900, '\t', Loss : 0.06832689791917801]\n",
            "[Step : 19000, '\t', Loss : 0.06894726306200027]\n",
            "[Step : 19100, '\t', Loss : 0.06424196809530258]\n",
            "[Step : 19200, '\t', Loss : 0.057082753628492355]\n",
            "[Step : 19300, '\t', Loss : 0.0984921008348465]\n",
            "[Epoch 8 / 30]\n",
            "[Step : 19400, '\t', Loss : 0.04477691650390625]\n",
            "[Step : 19500, '\t', Loss : 0.044964395463466644]\n",
            "[Step : 19600, '\t', Loss : 0.06177893280982971]\n",
            "[Step : 19700, '\t', Loss : 0.05553516745567322]\n",
            "[Step : 19800, '\t', Loss : 0.06514745205640793]\n",
            "[Step : 19900, '\t', Loss : 0.048550333827733994]\n",
            "[Step : 20000, '\t', Loss : 0.062031038105487823]\n",
            "[Step : 20100, '\t', Loss : 0.07551401853561401]\n",
            "[Step : 20200, '\t', Loss : 0.056342270225286484]\n",
            "[Step : 20300, '\t', Loss : 0.05597149208188057]\n",
            "[Step : 20400, '\t', Loss : 0.0771089643239975]\n",
            "[Step : 20500, '\t', Loss : 0.05297238752245903]\n",
            "[Step : 20600, '\t', Loss : 0.04403108358383179]\n",
            "[Step : 20700, '\t', Loss : 0.0621182881295681]\n",
            "[Step : 20800, '\t', Loss : 0.07066690921783447]\n",
            "[Step : 20900, '\t', Loss : 0.05614343658089638]\n",
            "[Step : 21000, '\t', Loss : 0.05818765237927437]\n",
            "[Step : 21100, '\t', Loss : 0.05407360568642616]\n",
            "[Step : 21200, '\t', Loss : 0.04561454430222511]\n",
            "[Step : 21300, '\t', Loss : 0.06645737588405609]\n",
            "[Step : 21400, '\t', Loss : 0.05158740282058716]\n",
            "[Step : 21500, '\t', Loss : 0.05273839086294174]\n",
            "[Step : 21600, '\t', Loss : 0.041976213455200195]\n",
            "[Step : 21700, '\t', Loss : 0.07709129899740219]\n",
            "[Step : 21800, '\t', Loss : 0.05302520841360092]\n",
            "[Step : 21900, '\t', Loss : 0.055890973657369614]\n",
            "[Step : 22000, '\t', Loss : 0.05338112264871597]\n",
            "[Step : 22100, '\t', Loss : 0.04914957284927368]\n",
            "[Epoch 9 / 30]\n",
            "[Step : 22200, '\t', Loss : 0.05091942474246025]\n",
            "[Step : 22300, '\t', Loss : 0.04651188477873802]\n",
            "[Step : 22400, '\t', Loss : 0.06328987330198288]\n",
            "[Step : 22500, '\t', Loss : 0.05542935058474541]\n",
            "[Step : 22600, '\t', Loss : 0.04832809790968895]\n",
            "[Step : 22700, '\t', Loss : 0.04026791453361511]\n",
            "[Step : 22800, '\t', Loss : 0.04793902859091759]\n",
            "[Step : 22900, '\t', Loss : 0.07752195000648499]\n",
            "[Step : 23000, '\t', Loss : 0.0620141327381134]\n",
            "[Step : 23100, '\t', Loss : 0.07218611240386963]\n",
            "[Step : 23200, '\t', Loss : 0.05389523133635521]\n",
            "[Step : 23300, '\t', Loss : 0.06639530509710312]\n",
            "[Step : 23400, '\t', Loss : 0.08581434935331345]\n",
            "[Step : 23500, '\t', Loss : 0.04840584099292755]\n",
            "[Step : 23600, '\t', Loss : 0.06527335941791534]\n",
            "[Step : 23700, '\t', Loss : 0.05835217610001564]\n",
            "[Step : 23800, '\t', Loss : 0.04285557195544243]\n",
            "[Step : 23900, '\t', Loss : 0.0668952688574791]\n",
            "[Step : 24000, '\t', Loss : 0.051992397755384445]\n",
            "[Step : 24100, '\t', Loss : 0.06655430793762207]\n",
            "[Step : 24200, '\t', Loss : 0.05291341245174408]\n",
            "[Step : 24300, '\t', Loss : 0.047377049922943115]\n",
            "[Step : 24400, '\t', Loss : 0.05319782719016075]\n",
            "[Step : 24500, '\t', Loss : 0.05903833732008934]\n",
            "[Step : 24600, '\t', Loss : 0.07347665727138519]\n",
            "[Step : 24700, '\t', Loss : 0.048412177711725235]\n",
            "[Step : 24800, '\t', Loss : 0.08343677967786789]\n",
            "[Step : 24900, '\t', Loss : 0.03980313241481781]\n",
            "[Epoch 10 / 30]\n",
            "[Step : 25000, '\t', Loss : 0.034852009266614914]\n",
            "[Step : 25100, '\t', Loss : 0.03770589083433151]\n",
            "[Step : 25200, '\t', Loss : 0.035989437252283096]\n",
            "[Step : 25300, '\t', Loss : 0.06458979099988937]\n",
            "[Step : 25400, '\t', Loss : 0.03806919604539871]\n",
            "[Step : 25500, '\t', Loss : 0.05357777327299118]\n",
            "[Step : 25600, '\t', Loss : 0.030427003279328346]\n",
            "[Step : 25700, '\t', Loss : 0.04967079684138298]\n",
            "[Step : 25800, '\t', Loss : 0.06355177611112595]\n",
            "[Step : 25900, '\t', Loss : 0.058275315910577774]\n",
            "[Step : 26000, '\t', Loss : 0.05828740820288658]\n",
            "[Step : 26100, '\t', Loss : 0.047236572951078415]\n",
            "[Step : 26200, '\t', Loss : 0.05987502261996269]\n",
            "[Step : 26300, '\t', Loss : 0.05563490092754364]\n",
            "[Step : 26400, '\t', Loss : 0.039588458836078644]\n",
            "[Step : 26500, '\t', Loss : 0.04321523383259773]\n",
            "[Step : 26600, '\t', Loss : 0.04223524406552315]\n",
            "[Step : 26700, '\t', Loss : 0.04012520611286163]\n",
            "[Step : 26800, '\t', Loss : 0.07185956090688705]\n",
            "[Step : 26900, '\t', Loss : 0.03869807720184326]\n",
            "[Step : 27000, '\t', Loss : 0.03855355829000473]\n",
            "[Step : 27100, '\t', Loss : 0.056491442024707794]\n",
            "[Step : 27200, '\t', Loss : 0.04984268918633461]\n",
            "[Step : 27300, '\t', Loss : 0.04456083104014397]\n",
            "[Step : 27400, '\t', Loss : 0.04942743480205536]\n",
            "[Step : 27500, '\t', Loss : 0.043994564563035965]\n",
            "[Step : 27600, '\t', Loss : 0.05105122923851013]\n",
            "[Epoch 11 / 30]\n",
            "[Step : 27700, '\t', Loss : 0.029983745887875557]\n",
            "[Step : 27800, '\t', Loss : 0.049686629325151443]\n",
            "[Step : 27900, '\t', Loss : 0.04391643404960632]\n",
            "[Step : 28000, '\t', Loss : 0.036757178604602814]\n",
            "[Step : 28100, '\t', Loss : 0.052183374762535095]\n",
            "[Step : 28200, '\t', Loss : 0.05322093516588211]\n",
            "[Step : 28300, '\t', Loss : 0.03742718696594238]\n",
            "[Step : 28400, '\t', Loss : 0.03683048486709595]\n",
            "[Step : 28500, '\t', Loss : 0.05533534288406372]\n",
            "[Step : 28600, '\t', Loss : 0.0548723004758358]\n",
            "[Step : 28700, '\t', Loss : 0.032585419714450836]\n",
            "[Step : 28800, '\t', Loss : 0.0467013344168663]\n",
            "[Step : 28900, '\t', Loss : 0.03453662618994713]\n",
            "[Step : 29000, '\t', Loss : 0.043458398431539536]\n",
            "[Step : 29100, '\t', Loss : 0.04212736710906029]\n",
            "[Step : 29200, '\t', Loss : 0.043886180967092514]\n",
            "[Step : 29300, '\t', Loss : 0.04149796441197395]\n",
            "[Step : 29400, '\t', Loss : 0.05819803103804588]\n",
            "[Step : 29500, '\t', Loss : 0.04368716478347778]\n",
            "[Step : 29600, '\t', Loss : 0.032742466777563095]\n",
            "[Step : 29700, '\t', Loss : 0.039428018033504486]\n",
            "[Step : 29800, '\t', Loss : 0.0649804100394249]\n",
            "[Step : 29900, '\t', Loss : 0.05566612631082535]\n",
            "[Step : 30000, '\t', Loss : 0.03708835691213608]\n",
            "[Step : 30100, '\t', Loss : 0.056158825755119324]\n",
            "[Step : 30200, '\t', Loss : 0.05236764997243881]\n",
            "[Step : 30300, '\t', Loss : 0.05413508787751198]\n",
            "[Step : 30400, '\t', Loss : 0.046184029430150986]\n",
            "[Epoch 12 / 30]\n",
            "[Step : 30500, '\t', Loss : 0.04522671177983284]\n",
            "[Step : 30600, '\t', Loss : 0.04723169654607773]\n",
            "[Step : 30700, '\t', Loss : 0.0415363684296608]\n",
            "[Step : 30800, '\t', Loss : 0.04010598361492157]\n",
            "[Step : 30900, '\t', Loss : 0.06039634719491005]\n",
            "[Step : 31000, '\t', Loss : 0.04593147337436676]\n",
            "[Step : 31100, '\t', Loss : 0.07192397117614746]\n",
            "[Step : 31200, '\t', Loss : 0.030879320576786995]\n",
            "[Step : 31300, '\t', Loss : 0.03213109076023102]\n",
            "[Step : 31400, '\t', Loss : 0.044945672154426575]\n",
            "[Step : 31500, '\t', Loss : 0.044871315360069275]\n",
            "[Step : 31600, '\t', Loss : 0.040476374328136444]\n",
            "[Step : 31700, '\t', Loss : 0.03967105224728584]\n",
            "[Step : 31800, '\t', Loss : 0.06178884953260422]\n",
            "[Step : 31900, '\t', Loss : 0.04929381608963013]\n",
            "[Step : 32000, '\t', Loss : 0.035151366144418716]\n",
            "[Step : 32100, '\t', Loss : 0.03787601366639137]\n",
            "[Step : 32200, '\t', Loss : 0.04758134111762047]\n",
            "[Step : 32300, '\t', Loss : 0.03418055921792984]\n",
            "[Step : 32400, '\t', Loss : 0.041073065251111984]\n",
            "[Step : 32500, '\t', Loss : 0.03987414762377739]\n",
            "[Step : 32600, '\t', Loss : 0.03926211968064308]\n",
            "[Step : 32700, '\t', Loss : 0.03023361787199974]\n",
            "[Step : 32800, '\t', Loss : 0.04640347510576248]\n",
            "[Step : 32900, '\t', Loss : 0.028649015352129936]\n",
            "[Step : 33000, '\t', Loss : 0.05592164769768715]\n",
            "[Step : 33100, '\t', Loss : 0.04616658017039299]\n",
            "[Step : 33200, '\t', Loss : 0.04608096554875374]\n",
            "[Epoch 13 / 30]\n",
            "[Step : 33300, '\t', Loss : 0.04940829426050186]\n",
            "[Step : 33400, '\t', Loss : 0.030797069892287254]\n",
            "[Step : 33500, '\t', Loss : 0.033444683998823166]\n",
            "[Step : 33600, '\t', Loss : 0.05990173667669296]\n",
            "[Step : 33700, '\t', Loss : 0.02973194047808647]\n",
            "[Step : 33800, '\t', Loss : 0.03625412657856941]\n",
            "[Step : 33900, '\t', Loss : 0.050515271723270416]\n",
            "[Step : 34000, '\t', Loss : 0.026305776089429855]\n",
            "[Step : 34100, '\t', Loss : 0.04422204941511154]\n",
            "[Step : 34200, '\t', Loss : 0.03800886496901512]\n",
            "[Step : 34300, '\t', Loss : 0.055289529263973236]\n",
            "[Step : 34400, '\t', Loss : 0.037533100694417953]\n",
            "[Step : 34500, '\t', Loss : 0.04563063755631447]\n",
            "[Step : 34600, '\t', Loss : 0.06094595417380333]\n",
            "[Step : 34700, '\t', Loss : 0.04679727181792259]\n",
            "[Step : 34800, '\t', Loss : 0.03544754907488823]\n",
            "[Step : 34900, '\t', Loss : 0.03004821017384529]\n",
            "[Step : 35000, '\t', Loss : 0.031126707792282104]\n",
            "[Step : 35100, '\t', Loss : 0.04534030333161354]\n",
            "[Step : 35200, '\t', Loss : 0.027853036299347878]\n",
            "[Step : 35300, '\t', Loss : 0.04432537034153938]\n",
            "[Step : 35400, '\t', Loss : 0.050131093710660934]\n",
            "[Step : 35500, '\t', Loss : 0.026702210307121277]\n",
            "[Step : 35600, '\t', Loss : 0.045532241463661194]\n",
            "[Step : 35700, '\t', Loss : 0.027762748301029205]\n",
            "[Step : 35800, '\t', Loss : 0.033345483243465424]\n",
            "[Step : 35900, '\t', Loss : 0.03281347453594208]\n",
            "[Epoch 14 / 30]\n",
            "[Step : 36000, '\t', Loss : 0.027002502232789993]\n",
            "[Step : 36100, '\t', Loss : 0.039213016629219055]\n",
            "[Step : 36200, '\t', Loss : 0.02624073438346386]\n",
            "[Step : 36300, '\t', Loss : 0.03892211616039276]\n",
            "[Step : 36400, '\t', Loss : 0.03409074991941452]\n",
            "[Step : 36500, '\t', Loss : 0.030882228165864944]\n",
            "[Step : 36600, '\t', Loss : 0.02721705101430416]\n",
            "[Step : 36700, '\t', Loss : 0.03147796913981438]\n",
            "[Step : 36800, '\t', Loss : 0.03125875070691109]\n",
            "[Step : 36900, '\t', Loss : 0.03687092289328575]\n",
            "[Step : 37000, '\t', Loss : 0.029577214270830154]\n",
            "[Step : 37100, '\t', Loss : 0.024081233888864517]\n",
            "[Step : 37200, '\t', Loss : 0.03157753869891167]\n",
            "[Step : 37300, '\t', Loss : 0.027382662519812584]\n",
            "[Step : 37400, '\t', Loss : 0.05192849412560463]\n",
            "[Step : 37500, '\t', Loss : 0.03987857326865196]\n",
            "[Step : 37600, '\t', Loss : 0.03545412793755531]\n",
            "[Step : 37700, '\t', Loss : 0.030722975730895996]\n",
            "[Step : 37800, '\t', Loss : 0.02322990447282791]\n",
            "[Step : 37900, '\t', Loss : 0.0315290130674839]\n",
            "[Step : 38000, '\t', Loss : 0.048754166811704636]\n",
            "[Step : 38100, '\t', Loss : 0.030090104788541794]\n",
            "[Step : 38200, '\t', Loss : 0.0632060095667839]\n",
            "[Step : 38300, '\t', Loss : 0.0269748754799366]\n",
            "[Step : 38400, '\t', Loss : 0.027840454131364822]\n",
            "[Step : 38500, '\t', Loss : 0.04949234053492546]\n",
            "[Step : 38600, '\t', Loss : 0.03847965598106384]\n",
            "[Step : 38700, '\t', Loss : 0.0316048227250576]\n",
            "[Epoch 15 / 30]\n",
            "[Step : 38800, '\t', Loss : 0.0409475602209568]\n",
            "[Step : 38900, '\t', Loss : 0.04073343798518181]\n",
            "[Step : 39000, '\t', Loss : 0.04626276344060898]\n",
            "[Step : 39100, '\t', Loss : 0.033516429364681244]\n",
            "[Step : 39200, '\t', Loss : 0.04447413235902786]\n",
            "[Step : 39300, '\t', Loss : 0.05786833539605141]\n",
            "[Step : 39400, '\t', Loss : 0.03930859640240669]\n",
            "[Step : 39500, '\t', Loss : 0.03894629329442978]\n",
            "[Step : 39600, '\t', Loss : 0.03661235049366951]\n",
            "[Step : 39700, '\t', Loss : 0.04207603633403778]\n",
            "[Step : 39800, '\t', Loss : 0.025363415479660034]\n",
            "[Step : 39900, '\t', Loss : 0.037697721272706985]\n",
            "[Step : 40000, '\t', Loss : 0.0412377305328846]\n",
            "[Step : 40100, '\t', Loss : 0.025824489071965218]\n",
            "[Step : 40200, '\t', Loss : 0.028420424088835716]\n",
            "[Step : 40300, '\t', Loss : 0.031986888498067856]\n",
            "[Step : 40400, '\t', Loss : 0.02913501113653183]\n",
            "[Step : 40500, '\t', Loss : 0.03324092552065849]\n",
            "[Step : 40600, '\t', Loss : 0.04876378923654556]\n",
            "[Step : 40700, '\t', Loss : 0.05378643050789833]\n",
            "[Step : 40800, '\t', Loss : 0.03924322500824928]\n",
            "[Step : 40900, '\t', Loss : 0.035149771720170975]\n",
            "[Step : 41000, '\t', Loss : 0.025580856949090958]\n",
            "[Step : 41100, '\t', Loss : 0.03386256843805313]\n",
            "[Step : 41200, '\t', Loss : 0.03039231337606907]\n",
            "[Step : 41300, '\t', Loss : 0.04163777455687523]\n",
            "[Step : 41400, '\t', Loss : 0.03474191576242447]\n",
            "[Step : 41500, '\t', Loss : 0.01930779591202736]\n",
            "[Epoch 16 / 30]\n",
            "[Step : 41600, '\t', Loss : 0.03512134775519371]\n",
            "[Step : 41700, '\t', Loss : 0.0299269687384367]\n",
            "[Step : 41800, '\t', Loss : 0.030968142673373222]\n",
            "[Step : 41900, '\t', Loss : 0.03853622451424599]\n",
            "[Step : 42000, '\t', Loss : 0.02103320322930813]\n",
            "[Step : 42100, '\t', Loss : 0.02505875751376152]\n",
            "[Step : 42200, '\t', Loss : 0.0345284640789032]\n",
            "[Step : 42300, '\t', Loss : 0.059058960527181625]\n",
            "[Step : 42400, '\t', Loss : 0.03550250828266144]\n",
            "[Step : 42500, '\t', Loss : 0.03224571421742439]\n",
            "[Step : 42600, '\t', Loss : 0.018117617815732956]\n",
            "[Step : 42700, '\t', Loss : 0.026541149243712425]\n",
            "[Step : 42800, '\t', Loss : 0.021463744342327118]\n",
            "[Step : 42900, '\t', Loss : 0.030728839337825775]\n",
            "[Step : 43000, '\t', Loss : 0.020598717033863068]\n",
            "[Step : 43100, '\t', Loss : 0.033564645797014236]\n",
            "[Step : 43200, '\t', Loss : 0.030742287635803223]\n",
            "[Step : 43300, '\t', Loss : 0.028500407934188843]\n",
            "[Step : 43400, '\t', Loss : 0.046312976628541946]\n",
            "[Step : 43500, '\t', Loss : 0.02448504976928234]\n",
            "[Step : 43600, '\t', Loss : 0.026830747723579407]\n",
            "[Step : 43700, '\t', Loss : 0.03320394083857536]\n",
            "[Step : 43800, '\t', Loss : 0.027054602280259132]\n",
            "[Step : 43900, '\t', Loss : 0.02580549381673336]\n",
            "[Step : 44000, '\t', Loss : 0.02979450859129429]\n",
            "[Step : 44100, '\t', Loss : 0.038107383996248245]\n",
            "[Step : 44200, '\t', Loss : 0.05545125901699066]\n",
            "[Epoch 17 / 30]\n",
            "[Step : 44300, '\t', Loss : 0.05909139662981033]\n",
            "[Step : 44400, '\t', Loss : 0.03396536037325859]\n",
            "[Step : 44500, '\t', Loss : 0.03333338350057602]\n",
            "[Step : 44600, '\t', Loss : 0.02390805073082447]\n",
            "[Step : 44700, '\t', Loss : 0.049646202474832535]\n",
            "[Step : 44800, '\t', Loss : 0.0287797749042511]\n",
            "[Step : 44900, '\t', Loss : 0.05302690342068672]\n",
            "[Step : 45000, '\t', Loss : 0.03125080093741417]\n",
            "[Step : 45100, '\t', Loss : 0.025800606235861778]\n",
            "[Step : 45200, '\t', Loss : 0.01875009387731552]\n",
            "[Step : 45300, '\t', Loss : 0.04135192185640335]\n",
            "[Step : 45400, '\t', Loss : 0.04089835658669472]\n",
            "[Step : 45500, '\t', Loss : 0.041119568049907684]\n",
            "[Step : 45600, '\t', Loss : 0.031320150941610336]\n",
            "[Step : 45700, '\t', Loss : 0.032802630215883255]\n",
            "[Step : 45800, '\t', Loss : 0.03296004608273506]\n",
            "[Step : 45900, '\t', Loss : 0.019345030188560486]\n",
            "[Step : 46000, '\t', Loss : 0.026477456092834473]\n",
            "[Step : 46100, '\t', Loss : 0.022755376994609833]\n",
            "[Step : 46200, '\t', Loss : 0.023261433467268944]\n",
            "[Step : 46300, '\t', Loss : 0.021246712654829025]\n",
            "[Step : 46400, '\t', Loss : 0.02326137013733387]\n",
            "[Step : 46500, '\t', Loss : 0.04115160554647446]\n",
            "[Step : 46600, '\t', Loss : 0.038257814943790436]\n",
            "[Step : 46700, '\t', Loss : 0.05983118712902069]\n",
            "[Step : 46800, '\t', Loss : 0.049235764890909195]\n",
            "[Step : 46900, '\t', Loss : 0.03941710293292999]\n",
            "[Step : 47000, '\t', Loss : 0.028680123388767242]\n",
            "[Epoch 18 / 30]\n",
            "[Step : 47100, '\t', Loss : 0.023810535669326782]\n",
            "[Step : 47200, '\t', Loss : 0.017441783100366592]\n",
            "[Step : 47300, '\t', Loss : 0.022906379774212837]\n",
            "[Step : 47400, '\t', Loss : 0.0473700575530529]\n",
            "[Step : 47500, '\t', Loss : 0.036486562341451645]\n",
            "[Step : 47600, '\t', Loss : 0.03187865391373634]\n",
            "[Step : 47700, '\t', Loss : 0.023012591525912285]\n",
            "[Step : 47800, '\t', Loss : 0.027943549677729607]\n",
            "[Step : 47900, '\t', Loss : 0.026146890595555305]\n",
            "[Step : 48000, '\t', Loss : 0.039118487387895584]\n",
            "[Step : 48100, '\t', Loss : 0.026248078793287277]\n",
            "[Step : 48200, '\t', Loss : 0.04283488169312477]\n",
            "[Step : 48300, '\t', Loss : 0.024769984185695648]\n",
            "[Step : 48400, '\t', Loss : 0.0299055352807045]\n",
            "[Step : 48500, '\t', Loss : 0.030298354104161263]\n",
            "[Step : 48600, '\t', Loss : 0.05056324973702431]\n",
            "[Step : 48700, '\t', Loss : 0.04252520576119423]\n",
            "[Step : 48800, '\t', Loss : 0.017970619723200798]\n",
            "[Step : 48900, '\t', Loss : 0.02320658601820469]\n",
            "[Step : 49000, '\t', Loss : 0.05517704039812088]\n",
            "[Step : 49100, '\t', Loss : 0.026148999109864235]\n",
            "[Step : 49200, '\t', Loss : 0.023210106417536736]\n",
            "[Step : 49300, '\t', Loss : 0.023297065868973732]\n",
            "[Step : 49400, '\t', Loss : 0.029972247779369354]\n",
            "[Step : 49500, '\t', Loss : 0.031226716935634613]\n",
            "[Step : 49600, '\t', Loss : 0.03820176050066948]\n",
            "[Step : 49700, '\t', Loss : 0.02512311190366745]\n",
            "[Step : 49800, '\t', Loss : 0.028873933479189873]\n",
            "[Epoch 19 / 30]\n",
            "[Step : 49900, '\t', Loss : 0.04644130542874336]\n",
            "[Step : 50000, '\t', Loss : 0.03759443387389183]\n",
            "[Step : 50100, '\t', Loss : 0.039800919592380524]\n",
            "[Step : 50200, '\t', Loss : 0.016955364495515823]\n",
            "[Step : 50300, '\t', Loss : 0.04516785964369774]\n",
            "[Step : 50400, '\t', Loss : 0.024991655722260475]\n",
            "[Step : 50500, '\t', Loss : 0.018083490431308746]\n",
            "[Step : 50600, '\t', Loss : 0.037938978523015976]\n",
            "[Step : 50700, '\t', Loss : 0.021760936826467514]\n",
            "[Step : 50800, '\t', Loss : 0.03247768059372902]\n",
            "[Step : 50900, '\t', Loss : 0.054096437990665436]\n",
            "[Step : 51000, '\t', Loss : 0.030849967151880264]\n",
            "[Step : 51100, '\t', Loss : 0.025556841865181923]\n",
            "[Step : 51200, '\t', Loss : 0.019193019717931747]\n",
            "[Step : 51300, '\t', Loss : 0.027393003925681114]\n",
            "[Step : 51400, '\t', Loss : 0.024939699098467827]\n",
            "[Step : 51500, '\t', Loss : 0.04388133063912392]\n",
            "[Step : 51600, '\t', Loss : 0.024554656818509102]\n",
            "[Step : 51700, '\t', Loss : 0.01717446558177471]\n",
            "[Step : 51800, '\t', Loss : 0.04365288093686104]\n",
            "[Step : 51900, '\t', Loss : 0.02105165645480156]\n",
            "[Step : 52000, '\t', Loss : 0.027763083577156067]\n",
            "[Step : 52100, '\t', Loss : 0.016177281737327576]\n",
            "[Step : 52200, '\t', Loss : 0.023743772879242897]\n",
            "[Step : 52300, '\t', Loss : 0.023929942399263382]\n",
            "[Step : 52400, '\t', Loss : 0.05508487671613693]\n",
            "[Step : 52500, '\t', Loss : 0.03276149928569794]\n",
            "[Epoch 20 / 30]\n",
            "[Step : 52600, '\t', Loss : 0.02161281555891037]\n",
            "[Step : 52700, '\t', Loss : 0.0215145256370306]\n",
            "[Step : 52800, '\t', Loss : 0.04502682387828827]\n",
            "[Step : 52900, '\t', Loss : 0.027071664109826088]\n",
            "[Step : 53000, '\t', Loss : 0.02081839181482792]\n",
            "[Step : 53100, '\t', Loss : 0.019936807453632355]\n",
            "[Step : 53200, '\t', Loss : 0.021164346486330032]\n",
            "[Step : 53300, '\t', Loss : 0.015304379165172577]\n",
            "[Step : 53400, '\t', Loss : 0.03762292116880417]\n",
            "[Step : 53500, '\t', Loss : 0.03963061422109604]\n",
            "[Step : 53600, '\t', Loss : 0.044402290135622025]\n",
            "[Step : 53700, '\t', Loss : 0.026806095615029335]\n",
            "[Step : 53800, '\t', Loss : 0.024736905470490456]\n",
            "[Step : 53900, '\t', Loss : 0.015407741069793701]\n",
            "[Step : 54000, '\t', Loss : 0.02931980788707733]\n",
            "[Step : 54100, '\t', Loss : 0.0270396675914526]\n",
            "[Step : 54200, '\t', Loss : 0.028497178107500076]\n",
            "[Step : 54300, '\t', Loss : 0.04341181740164757]\n",
            "[Step : 54400, '\t', Loss : 0.01575998030602932]\n",
            "[Step : 54500, '\t', Loss : 0.019475137814879417]\n",
            "[Step : 54600, '\t', Loss : 0.02880762703716755]\n",
            "[Step : 54700, '\t', Loss : 0.026370571926236153]\n",
            "[Step : 54800, '\t', Loss : 0.02291198819875717]\n",
            "[Step : 54900, '\t', Loss : 0.023288942873477936]\n",
            "[Step : 55000, '\t', Loss : 0.02308843284845352]\n",
            "[Step : 55100, '\t', Loss : 0.028610385954380035]\n",
            "[Step : 55200, '\t', Loss : 0.030507246032357216]\n",
            "[Step : 55300, '\t', Loss : 0.020750099793076515]\n",
            "[Epoch 21 / 30]\n",
            "[Step : 55400, '\t', Loss : 0.03531096503138542]\n",
            "[Step : 55500, '\t', Loss : 0.022900139912962914]\n",
            "[Step : 55600, '\t', Loss : 0.02877006120979786]\n",
            "[Step : 55700, '\t', Loss : 0.014963621273636818]\n",
            "[Step : 55800, '\t', Loss : 0.027956826612353325]\n",
            "[Step : 55900, '\t', Loss : 0.0294504277408123]\n",
            "[Step : 56000, '\t', Loss : 0.04681786522269249]\n",
            "[Step : 56100, '\t', Loss : 0.020564483478665352]\n",
            "[Step : 56200, '\t', Loss : 0.016706574708223343]\n",
            "[Step : 56300, '\t', Loss : 0.012814909219741821]\n",
            "[Step : 56400, '\t', Loss : 0.01673707365989685]\n",
            "[Step : 56500, '\t', Loss : 0.03086947835981846]\n",
            "[Step : 56600, '\t', Loss : 0.019351519644260406]\n",
            "[Step : 56700, '\t', Loss : 0.02099979855120182]\n",
            "[Step : 56800, '\t', Loss : 0.01632268726825714]\n",
            "[Step : 56900, '\t', Loss : 0.01798797771334648]\n",
            "[Step : 57000, '\t', Loss : 0.04158088192343712]\n",
            "[Step : 57100, '\t', Loss : 0.021045809611678123]\n",
            "[Step : 57200, '\t', Loss : 0.021671168506145477]\n",
            "[Step : 57300, '\t', Loss : 0.019482817500829697]\n",
            "[Step : 57400, '\t', Loss : 0.021415991708636284]\n",
            "[Step : 57500, '\t', Loss : 0.0406372956931591]\n",
            "[Step : 57600, '\t', Loss : 0.035804037004709244]\n",
            "[Step : 57700, '\t', Loss : 0.038665395230054855]\n",
            "[Step : 57800, '\t', Loss : 0.014164108783006668]\n",
            "[Step : 57900, '\t', Loss : 0.03006780706346035]\n",
            "[Step : 58000, '\t', Loss : 0.01848493702709675]\n",
            "[Step : 58100, '\t', Loss : 0.030188217759132385]\n",
            "[Epoch 22 / 30]\n",
            "[Step : 58200, '\t', Loss : 0.01674092747271061]\n",
            "[Step : 58300, '\t', Loss : 0.01675833761692047]\n",
            "[Step : 58400, '\t', Loss : 0.046659331768751144]\n",
            "[Step : 58500, '\t', Loss : 0.03650463744997978]\n",
            "[Step : 58600, '\t', Loss : 0.02780730649828911]\n",
            "[Step : 58700, '\t', Loss : 0.05058618634939194]\n",
            "[Step : 58800, '\t', Loss : 0.024595066905021667]\n",
            "[Step : 58900, '\t', Loss : 0.013579225167632103]\n",
            "[Step : 59000, '\t', Loss : 0.030055692419409752]\n",
            "[Step : 59100, '\t', Loss : 0.028653757646679878]\n",
            "[Step : 59200, '\t', Loss : 0.01930803619325161]\n",
            "[Step : 59300, '\t', Loss : 0.033011533319950104]\n",
            "[Step : 59400, '\t', Loss : 0.01811426877975464]\n",
            "[Step : 59500, '\t', Loss : 0.037239931523799896]\n",
            "[Step : 59600, '\t', Loss : 0.02525622583925724]\n",
            "[Step : 59700, '\t', Loss : 0.026982516050338745]\n",
            "[Step : 59800, '\t', Loss : 0.021197179332375526]\n",
            "[Step : 59900, '\t', Loss : 0.027537425979971886]\n",
            "[Step : 60000, '\t', Loss : 0.023434719070792198]\n",
            "[Step : 60100, '\t', Loss : 0.015181062743067741]\n",
            "[Step : 60200, '\t', Loss : 0.01740005426108837]\n",
            "[Step : 60300, '\t', Loss : 0.032398782670497894]\n",
            "[Step : 60400, '\t', Loss : 0.030936649069190025]\n",
            "[Step : 60500, '\t', Loss : 0.027017898857593536]\n",
            "[Step : 60600, '\t', Loss : 0.05011207237839699]\n",
            "[Step : 60700, '\t', Loss : 0.02011178806424141]\n",
            "[Step : 60800, '\t', Loss : 0.022234586998820305]\n",
            "[Epoch 23 / 30]\n",
            "[Step : 60900, '\t', Loss : 0.036367204040288925]\n",
            "[Step : 61000, '\t', Loss : 0.02293558605015278]\n",
            "[Step : 61100, '\t', Loss : 0.04200955480337143]\n",
            "[Step : 61200, '\t', Loss : 0.024331271648406982]\n",
            "[Step : 61300, '\t', Loss : 0.022954793646931648]\n",
            "[Step : 61400, '\t', Loss : 0.04170603305101395]\n",
            "[Step : 61500, '\t', Loss : 0.010270342230796814]\n",
            "[Step : 61600, '\t', Loss : 0.02394876815378666]\n",
            "[Step : 61700, '\t', Loss : 0.027254441753029823]\n",
            "[Step : 61800, '\t', Loss : 0.016305692493915558]\n",
            "[Step : 61900, '\t', Loss : 0.02494150958955288]\n",
            "[Step : 62000, '\t', Loss : 0.027254268527030945]\n",
            "[Step : 62100, '\t', Loss : 0.025882471352815628]\n",
            "[Step : 62200, '\t', Loss : 0.03668953850865364]\n",
            "[Step : 62300, '\t', Loss : 0.030468208715319633]\n",
            "[Step : 62400, '\t', Loss : 0.02610190212726593]\n",
            "[Step : 62500, '\t', Loss : 0.020163491368293762]\n",
            "[Step : 62600, '\t', Loss : 0.014156016521155834]\n",
            "[Step : 62700, '\t', Loss : 0.020217780023813248]\n",
            "[Step : 62800, '\t', Loss : 0.029294859617948532]\n",
            "[Step : 62900, '\t', Loss : 0.01944025233387947]\n",
            "[Step : 63000, '\t', Loss : 0.05437459796667099]\n",
            "[Step : 63100, '\t', Loss : 0.012944693677127361]\n",
            "[Step : 63200, '\t', Loss : 0.01711113192141056]\n",
            "[Step : 63300, '\t', Loss : 0.032301586121320724]\n",
            "[Step : 63400, '\t', Loss : 0.028395945206284523]\n",
            "[Step : 63500, '\t', Loss : 0.016494253650307655]\n",
            "[Step : 63600, '\t', Loss : 0.02173761837184429]\n",
            "[Epoch 24 / 30]\n",
            "[Step : 63700, '\t', Loss : 0.013087909668684006]\n",
            "[Step : 63800, '\t', Loss : 0.02227623015642166]\n",
            "[Step : 63900, '\t', Loss : 0.03662683442234993]\n",
            "[Step : 64000, '\t', Loss : 0.03178252652287483]\n",
            "[Step : 64100, '\t', Loss : 0.010408816859126091]\n",
            "[Step : 64200, '\t', Loss : 0.015174943953752518]\n",
            "[Step : 64300, '\t', Loss : 0.01259352546185255]\n",
            "[Step : 64400, '\t', Loss : 0.013781802728772163]\n",
            "[Step : 64500, '\t', Loss : 0.02062438800930977]\n",
            "[Step : 64600, '\t', Loss : 0.016749504953622818]\n",
            "[Step : 64700, '\t', Loss : 0.03300389647483826]\n",
            "[Step : 64800, '\t', Loss : 0.016475610435009003]\n",
            "[Step : 64900, '\t', Loss : 0.027517955750226974]\n",
            "[Step : 65000, '\t', Loss : 0.0381338968873024]\n",
            "[Step : 65100, '\t', Loss : 0.019241346046328545]\n",
            "[Step : 65200, '\t', Loss : 0.019996782764792442]\n",
            "[Step : 65300, '\t', Loss : 0.039840083569288254]\n",
            "[Step : 65400, '\t', Loss : 0.014598785899579525]\n",
            "[Step : 65500, '\t', Loss : 0.022749565541744232]\n",
            "[Step : 65600, '\t', Loss : 0.024812864139676094]\n",
            "[Step : 65700, '\t', Loss : 0.019825546070933342]\n",
            "[Step : 65800, '\t', Loss : 0.03906532749533653]\n",
            "[Step : 65900, '\t', Loss : 0.023088425397872925]\n",
            "[Step : 66000, '\t', Loss : 0.03343097120523453]\n",
            "[Step : 66100, '\t', Loss : 0.03848106041550636]\n",
            "[Step : 66200, '\t', Loss : 0.020048849284648895]\n",
            "[Step : 66300, '\t', Loss : 0.061485759913921356]\n",
            "[Step : 66400, '\t', Loss : 0.03346942365169525]\n",
            "[Epoch 25 / 30]\n",
            "[Step : 66500, '\t', Loss : 0.046885307878255844]\n",
            "[Step : 66600, '\t', Loss : 0.03440367802977562]\n",
            "[Step : 66700, '\t', Loss : 0.021542051807045937]\n",
            "[Step : 66800, '\t', Loss : 0.01988202892243862]\n",
            "[Step : 66900, '\t', Loss : 0.013284921646118164]\n",
            "[Step : 67000, '\t', Loss : 0.03228292241692543]\n",
            "[Step : 67100, '\t', Loss : 0.015660108998417854]\n",
            "[Step : 67200, '\t', Loss : 0.02638421766459942]\n",
            "[Step : 67300, '\t', Loss : 0.021035902202129364]\n",
            "[Step : 67400, '\t', Loss : 0.022182287648320198]\n",
            "[Step : 67500, '\t', Loss : 0.03175485506653786]\n",
            "[Step : 67600, '\t', Loss : 0.015663189813494682]\n",
            "[Step : 67700, '\t', Loss : 0.033841103315353394]\n",
            "[Step : 67800, '\t', Loss : 0.018833253532648087]\n",
            "[Step : 67900, '\t', Loss : 0.014230706728994846]\n",
            "[Step : 68000, '\t', Loss : 0.056673187762498856]\n",
            "[Step : 68100, '\t', Loss : 0.018809860572218895]\n",
            "[Step : 68200, '\t', Loss : 0.014847733080387115]\n",
            "[Step : 68300, '\t', Loss : 0.01110912300646305]\n",
            "[Step : 68400, '\t', Loss : 0.00837667752057314]\n",
            "[Step : 68500, '\t', Loss : 0.013148355297744274]\n",
            "[Step : 68600, '\t', Loss : 0.03162441402673721]\n",
            "[Step : 68700, '\t', Loss : 0.018729502335190773]\n",
            "[Step : 68800, '\t', Loss : 0.0240198764950037]\n",
            "[Step : 68900, '\t', Loss : 0.015728766098618507]\n",
            "[Step : 69000, '\t', Loss : 0.02881167270243168]\n",
            "[Step : 69100, '\t', Loss : 0.04765399917960167]\n",
            "[Step : 69200, '\t', Loss : 0.018344484269618988]\n",
            "[Epoch 26 / 30]\n",
            "[Step : 69300, '\t', Loss : 0.013248300179839134]\n",
            "[Step : 69400, '\t', Loss : 0.022764703258872032]\n",
            "[Step : 69500, '\t', Loss : 0.014232452027499676]\n",
            "[Step : 69600, '\t', Loss : 0.013851337134838104]\n",
            "[Step : 69700, '\t', Loss : 0.014165769331157207]\n",
            "[Step : 69800, '\t', Loss : 0.032877229154109955]\n",
            "[Step : 69900, '\t', Loss : 0.023127010092139244]\n",
            "[Step : 70000, '\t', Loss : 0.04838098585605621]\n",
            "[Step : 70100, '\t', Loss : 0.026297111064195633]\n",
            "[Step : 70200, '\t', Loss : 0.03932560235261917]\n",
            "[Step : 70300, '\t', Loss : 0.01776561141014099]\n",
            "[Step : 70400, '\t', Loss : 0.014746439643204212]\n",
            "[Step : 70500, '\t', Loss : 0.031410783529281616]\n",
            "[Step : 70600, '\t', Loss : 0.00981482770293951]\n",
            "[Step : 70700, '\t', Loss : 0.017623087391257286]\n",
            "[Step : 70800, '\t', Loss : 0.014508872292935848]\n",
            "[Step : 70900, '\t', Loss : 0.015820695087313652]\n",
            "[Step : 71000, '\t', Loss : 0.021486831828951836]\n",
            "[Step : 71100, '\t', Loss : 0.015914367511868477]\n",
            "[Step : 71200, '\t', Loss : 0.02335898019373417]\n",
            "[Step : 71300, '\t', Loss : 0.018869774416089058]\n",
            "[Step : 71400, '\t', Loss : 0.021366773173213005]\n",
            "[Step : 71500, '\t', Loss : 0.013683975674211979]\n",
            "[Step : 71600, '\t', Loss : 0.018193505704402924]\n",
            "[Step : 71700, '\t', Loss : 0.02701210044324398]\n",
            "[Step : 71800, '\t', Loss : 0.0314842090010643]\n",
            "[Step : 71900, '\t', Loss : 0.02078964188694954]\n",
            "[Epoch 27 / 30]\n",
            "[Step : 72000, '\t', Loss : 0.023302193731069565]\n",
            "[Step : 72100, '\t', Loss : 0.015100655145943165]\n",
            "[Step : 72200, '\t', Loss : 0.010495033115148544]\n",
            "[Step : 72300, '\t', Loss : 0.008657734841108322]\n",
            "[Step : 72400, '\t', Loss : 0.006565324496477842]\n",
            "[Step : 72500, '\t', Loss : 0.01989573985338211]\n",
            "[Step : 72600, '\t', Loss : 0.009494590573012829]\n",
            "[Step : 72700, '\t', Loss : 0.023056359961628914]\n",
            "[Step : 72800, '\t', Loss : 0.017835889011621475]\n",
            "[Step : 72900, '\t', Loss : 0.03769468143582344]\n",
            "[Step : 73000, '\t', Loss : 0.032540224492549896]\n",
            "[Step : 73100, '\t', Loss : 0.032297443598508835]\n",
            "[Step : 73200, '\t', Loss : 0.03831582888960838]\n",
            "[Step : 73300, '\t', Loss : 0.02315215766429901]\n",
            "[Step : 73400, '\t', Loss : 0.013857451267540455]\n",
            "[Step : 73500, '\t', Loss : 0.010773761197924614]\n",
            "[Step : 73600, '\t', Loss : 0.029119323939085007]\n",
            "[Step : 73700, '\t', Loss : 0.02349551022052765]\n",
            "[Step : 73800, '\t', Loss : 0.02247745729982853]\n",
            "[Step : 73900, '\t', Loss : 0.012218366377055645]\n",
            "[Step : 74000, '\t', Loss : 0.016004296019673347]\n",
            "[Step : 74100, '\t', Loss : 0.018087610602378845]\n",
            "[Step : 74200, '\t', Loss : 0.03001312166452408]\n",
            "[Step : 74300, '\t', Loss : 0.023183906450867653]\n",
            "[Step : 74400, '\t', Loss : 0.011689211241900921]\n",
            "[Step : 74500, '\t', Loss : 0.021083159372210503]\n",
            "[Step : 74600, '\t', Loss : 0.03896065428853035]\n",
            "[Step : 74700, '\t', Loss : 0.041472602635622025]\n",
            "[Epoch 28 / 30]\n",
            "[Step : 74800, '\t', Loss : 0.015526275150477886]\n",
            "[Step : 74900, '\t', Loss : 0.024328170344233513]\n",
            "[Step : 75000, '\t', Loss : 0.011567034758627415]\n",
            "[Step : 75100, '\t', Loss : 0.022225696593523026]\n",
            "[Step : 75200, '\t', Loss : 0.01064099557697773]\n",
            "[Step : 75300, '\t', Loss : 0.012477248907089233]\n",
            "[Step : 75400, '\t', Loss : 0.018633684143424034]\n",
            "[Step : 75500, '\t', Loss : 0.02425895817577839]\n",
            "[Step : 75600, '\t', Loss : 0.0158038679510355]\n",
            "[Step : 75700, '\t', Loss : 0.018623651936650276]\n",
            "[Step : 75800, '\t', Loss : 0.02478279545903206]\n",
            "[Step : 75900, '\t', Loss : 0.0217574555426836]\n",
            "[Step : 76000, '\t', Loss : 0.0200357586145401]\n",
            "[Step : 76100, '\t', Loss : 0.022157395258545876]\n",
            "[Step : 76200, '\t', Loss : 0.015297447331249714]\n",
            "[Step : 76300, '\t', Loss : 0.02143358252942562]\n",
            "[Step : 76400, '\t', Loss : 0.029632559046149254]\n",
            "[Step : 76500, '\t', Loss : 0.019529566168785095]\n",
            "[Step : 76600, '\t', Loss : 0.017776185646653175]\n",
            "[Step : 76700, '\t', Loss : 0.017797691747546196]\n",
            "[Step : 76800, '\t', Loss : 0.025110067799687386]\n",
            "[Step : 76900, '\t', Loss : 0.021425606682896614]\n",
            "[Step : 77000, '\t', Loss : 0.028550071641802788]\n",
            "[Step : 77100, '\t', Loss : 0.02638123370707035]\n",
            "[Step : 77200, '\t', Loss : 0.036919090896844864]\n",
            "[Step : 77300, '\t', Loss : 0.023307034745812416]\n",
            "[Step : 77400, '\t', Loss : 0.03577607125043869]\n",
            "[Step : 77500, '\t', Loss : 0.0209360271692276]\n",
            "[Epoch 29 / 30]\n",
            "[Step : 77600, '\t', Loss : 0.02539566159248352]\n",
            "[Step : 77700, '\t', Loss : 0.02024873159825802]\n",
            "[Step : 77800, '\t', Loss : 0.03179660812020302]\n",
            "[Step : 77900, '\t', Loss : 0.01580939069390297]\n",
            "[Step : 78000, '\t', Loss : 0.020508935675024986]\n",
            "[Step : 78100, '\t', Loss : 0.018978195264935493]\n",
            "[Step : 78200, '\t', Loss : 0.02503153681755066]\n",
            "[Step : 78300, '\t', Loss : 0.019776707515120506]\n",
            "[Step : 78400, '\t', Loss : 0.019720641896128654]\n",
            "[Step : 78500, '\t', Loss : 0.026035012677311897]\n",
            "[Step : 78600, '\t', Loss : 0.02278721146285534]\n",
            "[Step : 78700, '\t', Loss : 0.025658946484327316]\n",
            "[Step : 78800, '\t', Loss : 0.025816069915890694]\n",
            "[Step : 78900, '\t', Loss : 0.018025724217295647]\n",
            "[Step : 79000, '\t', Loss : 0.012829368002712727]\n",
            "[Step : 79100, '\t', Loss : 0.028173627331852913]\n",
            "[Step : 79200, '\t', Loss : 0.016828393563628197]\n",
            "[Step : 79300, '\t', Loss : 0.02652745135128498]\n",
            "[Step : 79400, '\t', Loss : 0.012929721735417843]\n",
            "[Step : 79500, '\t', Loss : 0.025148391723632812]\n",
            "[Step : 79600, '\t', Loss : 0.04314981773495674]\n",
            "[Step : 79700, '\t', Loss : 0.01812729798257351]\n",
            "[Step : 79800, '\t', Loss : 0.015562573447823524]\n",
            "[Step : 79900, '\t', Loss : 0.026346154510974884]\n",
            "[Step : 80000, '\t', Loss : 0.043693527579307556]\n",
            "[Step : 80100, '\t', Loss : 0.037367552518844604]\n",
            "[Step : 80200, '\t', Loss : 0.022062189877033234]\n",
            "[Epoch 30 / 30]\n",
            "[Step : 80300, '\t', Loss : 0.04053628072142601]\n",
            "[Step : 80400, '\t', Loss : 0.013040147721767426]\n",
            "[Step : 80500, '\t', Loss : 0.020664745941758156]\n",
            "[Step : 80600, '\t', Loss : 0.01674855500459671]\n",
            "[Step : 80700, '\t', Loss : 0.044952914118766785]\n",
            "[Step : 80800, '\t', Loss : 0.010874388739466667]\n",
            "[Step : 80900, '\t', Loss : 0.024690240621566772]\n",
            "[Step : 81000, '\t', Loss : 0.035513781011104584]\n",
            "[Step : 81100, '\t', Loss : 0.012787292711436749]\n",
            "[Step : 81200, '\t', Loss : 0.021225333213806152]\n",
            "[Step : 81300, '\t', Loss : 0.011405818164348602]\n",
            "[Step : 81400, '\t', Loss : 0.027282385155558586]\n",
            "[Step : 81500, '\t', Loss : 0.01104229036718607]\n",
            "[Step : 81600, '\t', Loss : 0.026233196258544922]\n",
            "[Step : 81700, '\t', Loss : 0.015762023627758026]\n",
            "[Step : 81800, '\t', Loss : 0.017572347074747086]\n",
            "[Step : 81900, '\t', Loss : 0.026984544470906258]\n",
            "[Step : 82000, '\t', Loss : 0.02166646718978882]\n",
            "[Step : 82100, '\t', Loss : 0.015139499679207802]\n",
            "[Step : 82200, '\t', Loss : 0.028985928744077682]\n",
            "[Step : 82300, '\t', Loss : 0.020083870738744736]\n",
            "[Step : 82400, '\t', Loss : 0.011458012275397778]\n",
            "[Step : 82500, '\t', Loss : 0.02771816961467266]\n",
            "[Step : 82600, '\t', Loss : 0.024254102259874344]\n",
            "[Step : 82700, '\t', Loss : 0.019797151908278465]\n",
            "[Step : 82800, '\t', Loss : 0.019316844642162323]\n",
            "[Step : 82900, '\t', Loss : 0.018787691369652748]\n",
            "[Step : 83000, '\t', Loss : 0.014268042519688606]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZgGvvhomloC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd111255-4ed2-43c6-8286-fd41e5e287ba"
      },
      "source": [
        "print_every = 20\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    step = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(valid_iter):\n",
        "            input_data = batch.text.to(device)\n",
        "            targ = batch.headlines.to(device)\n",
        "\n",
        "            output = model(input_data, targ, teacher_forcing_ratio = 0)\n",
        "\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            targ = targ[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, targ)\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            if step % print_every == 0:\n",
        "                print(f\"[Step : {step}, Loss : {loss.item()/batch_size}]\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n",
            "[Step : 200, Loss : 0.2541641891002655]\n",
            "[Step : 220, Loss : 0.2534291446208954]\n",
            "[Step : 240, Loss : 0.25864899158477783]\n",
            "[Step : 260, Loss : 0.2828618586063385]\n",
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n",
            "[Step : 200, Loss : 0.2541641891002655]\n",
            "[Step : 220, Loss : 0.2534291446208954]\n",
            "[Step : 240, Loss : 0.25864899158477783]\n",
            "[Step : 260, Loss : 0.2828618586063385]\n",
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n",
            "[Step : 200, Loss : 0.2541641891002655]\n",
            "[Step : 220, Loss : 0.2534291446208954]\n",
            "[Step : 240, Loss : 0.25864899158477783]\n",
            "[Step : 260, Loss : 0.2828618586063385]\n",
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n",
            "[Step : 200, Loss : 0.2541641891002655]\n",
            "[Step : 220, Loss : 0.2534291446208954]\n",
            "[Step : 240, Loss : 0.25864899158477783]\n",
            "[Step : 260, Loss : 0.2828618586063385]\n",
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n",
            "[Step : 200, Loss : 0.2541641891002655]\n",
            "[Step : 220, Loss : 0.2534291446208954]\n",
            "[Step : 240, Loss : 0.25864899158477783]\n",
            "[Step : 260, Loss : 0.2828618586063385]\n",
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n",
            "[Step : 200, Loss : 0.2541641891002655]\n",
            "[Step : 220, Loss : 0.2534291446208954]\n",
            "[Step : 240, Loss : 0.25864899158477783]\n",
            "[Step : 260, Loss : 0.2828618586063385]\n",
            "[Step : 20, Loss : 0.2658458948135376]\n",
            "[Step : 40, Loss : 0.2695620059967041]\n",
            "[Step : 60, Loss : 0.2668425440788269]\n",
            "[Step : 80, Loss : 0.2606973946094513]\n",
            "[Step : 100, Loss : 0.23340405523777008]\n",
            "[Step : 120, Loss : 0.2436288446187973]\n",
            "[Step : 140, Loss : 0.26474714279174805]\n",
            "[Step : 160, Loss : 0.25943365693092346]\n",
            "[Step : 180, Loss : 0.2434902787208557]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e3f17383310c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7n96pmAppzZ",
        "colab_type": "text"
      },
      "source": [
        "# **Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J7t4F_uoNFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sentence, source_field, target_field, model, device, max_len = 20):\n",
        "    model.eval()\n",
        "\n",
        "    if sentence is str:\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [source_field.init_token] + tokens + [source_field.eos_token]\n",
        "    source_idxs = [source_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    source_tensor = torch.LongTensor(source_idxs).unsqueeze(dim = 1).to(device)\n",
        "    #source_len = torch.LongTensor([len(source_idxs)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(source_tensor)\n",
        "    \n",
        "    target_idxs = [target_field.vocab.stoi[target_field.init_token]]\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        target_tensor = torch.LongTensor([target_idxs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions, hidden = model.decoder(target_tensor, encoder_outputs, hidden)\n",
        "        \n",
        "        pred_token = predictions.argmax(1).item()\n",
        "        target_idxs.append(pred_token)\n",
        "\n",
        "        if pred_token == target_field.vocab.stoi[target_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    target_tokens = [target_field.vocab.itos[i] for i in target_idxs]\n",
        "\n",
        "    return target_tokens[1:]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIzm7pVlGteO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "14846517-465c-4cb7-ed43-b9c9dffe44d3"
      },
      "source": [
        "random_idx = random.randint(1, len(test.examples)+1)\n",
        "sample_text = vars(test.examples[random_idx])['text']\n",
        "sample_headline = vars(test.examples[random_idx])['headlines']\n",
        "predicted_headline = predict(sample_text, source, target, model, device)\n",
        "\n",
        "print(\"Text : \", \" \".join(word for word in sample_text))\n",
        "print(\"Headline : \", \" \".join(word for word in sample_headline))\n",
        "print(\"Predicted Headline : \", \" \".join(word for word in predicted_headline))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text :  hindu jagarana vedike leader jagadish karanth has reportedly been booked for making provocative comments and promoting enmity on grounds of religion during a protest meet in mangaluru , among other charges . karanth allegedly said police officer khaled should be \" stripped naked and made to run around in the bus stop \" , while claiming that khaled had illegally broken into a lawyer 's residence .\n",
            "Headline :  hindu jagarana vedike leader booked for provocative speech\n",
            "Predicted Headline :  kashmir leader leader sacked over making protests <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MLnNQxJMUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": []
    }
  ]
}